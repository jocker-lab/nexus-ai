# -*- coding: utf-8 -*-
"""
@File    :   nodes.py
@Time    :   2025/10/31 20:42
@Author  :   pygao
@Version :   1.0
@Contact :   pygao.1@outlook.com
@License :   (C)Copyright 2025, GienTech Technology Co.,Ltd. All rights reserved.
@Desc    :   é‡æ„ä¸ºç‹¬ç«‹èŠ‚ç‚¹æ¶æ„ - æ¯ä¸ªèŠ‚ç‚¹æœ‰æ¸…æ™°çš„æ—¥å¿—æ ‡è¯†
"""
import json
from loguru import logger
from langchain.chat_models import init_chat_model
from langgraph.types import interrupt, Command
from typing import Literal

from langgraph.graph import END

from langchain.messages import AIMessage
from app.agents.core.publisher.blueprint.state import PlanExecuteState
from app.agents.schemas.blueprint_schema import Plan, StepType, Step, StepExecution, ReplanSteps, \
    CoordinatorDecision
from app.agents.prompts.publisher_prompts.planner.replanner_prompt import replanner_prompt
from app.agents.prompts.template import apply_prompt_template


# ==================== æ¨¡ç‰ˆæœç´¢æ‰§è¡ŒèŠ‚ç‚¹ ====================

async def execute_template_search_node(state: PlanExecuteState):
    """
    ğŸ” æ¨¡ç‰ˆæœç´¢æ‰§è¡ŒèŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·éœ€æ±‚è¯­ä¹‰æœç´¢åŒ¹é…çš„æ¨¡ç‰ˆ

    ä½œä¸º StepType.TEMPLATE_SEARCH çš„æ‰§è¡ŒèŠ‚ç‚¹ï¼Œä¸å…¶ä»–æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆresearchã€human_involvementï¼‰åŒçº§

    æµç¨‹ï¼š
    1. ä» pending_steps è·å–å½“å‰æ­¥éª¤
    2. ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–éœ€æ±‚
    3. è°ƒç”¨ search_templates è¿›è¡Œè¯­ä¹‰æœç´¢
    4. è¿”å›æœç´¢ç»“æœï¼ˆæ¨¡ç‰ˆåˆ—è¡¨ï¼‰ï¼Œå­˜å…¥ completed_steps
    5. å¦‚æœæ‰¾åˆ°å¤šä¸ªæ¨¡ç‰ˆï¼ŒPlanner å¯ä»¥è§„åˆ’åç»­çš„ HUMAN_INVOLVEMENT è®©ç”¨æˆ·é€‰æ‹©

    è¿”å›ï¼š
    - pending_steps: ç§»é™¤å½“å‰æ­¥éª¤åçš„å‰©ä½™æ­¥éª¤
    - completed_steps: æ·»åŠ å½“å‰æ­¥éª¤çš„æ‰§è¡Œè®°å½•
    - matched_template: å¦‚æœåªæ‰¾åˆ°ä¸€ä¸ªé«˜ç›¸ä¼¼åº¦æ¨¡ç‰ˆï¼Œç›´æ¥è®¾ç½®ï¼›å¦åˆ™ä¸º None
    """
    logger.info("=" * 60)
    logger.info("ğŸ” [TEMPLATE SEARCH NODE] èŠ‚ç‚¹å¯åŠ¨")
    logger.info("=" * 60)

    from app.service.template_service import search_templates

    pending_steps = state.get("pending_steps", [])

    if not pending_steps:
        logger.warning("âš ï¸  [TEMPLATE SEARCH NODE] æ²¡æœ‰å¾…æ‰§è¡Œçš„æ­¥éª¤")
        return {"pending_steps": []}

    # è·å–å½“å‰æ­¥éª¤
    current_step = pending_steps[0]
    remaining_steps = pending_steps[1:]

    logger.info(f"ğŸ“Œ [TEMPLATE SEARCH NODE] å½“å‰ä»»åŠ¡: {current_step.target}")

    # æå–ç”¨æˆ·éœ€æ±‚æ–‡æœ¬ï¼ˆä» actions æˆ– conversation_messagesï¼‰
    search_query = ""
    if isinstance(current_step.actions, str):
        search_query = current_step.actions
    elif isinstance(current_step.actions, list) and current_step.actions:
        search_query = current_step.actions[0]

    # å¦‚æœ actions ä¸ºç©ºï¼Œä»å¯¹è¯æ¶ˆæ¯ä¸­æå–
    if not search_query:
        conversation_messages = state.get("conversation_messages", [])
        for msg in conversation_messages:
            if hasattr(msg, 'content') and hasattr(msg, 'type') and msg.type == 'human':
                search_query = msg.content
                break
            elif hasattr(msg, 'content') and not hasattr(msg, 'type'):
                search_query = msg.content
                break

    if not search_query:
        logger.warning("âš ï¸  [TEMPLATE SEARCH NODE] æœªæ‰¾åˆ°æœç´¢æŸ¥è¯¢ï¼Œè·³è¿‡æ¨¡ç‰ˆæœç´¢")
        execution = StepExecution(
            step=current_step,
            execution_res="æœªæ‰¾åˆ°æœç´¢æŸ¥è¯¢ï¼Œè·³è¿‡æ¨¡ç‰ˆæœç´¢",
            status="skipped"
        )
        return {
            "pending_steps": remaining_steps,
            "completed_steps": [execution],
            "matched_template": None
        }

    logger.info(f"   ğŸ“ æœç´¢æŸ¥è¯¢: {search_query[:100]}{'...' if len(search_query) > 100 else ''}")

    # æœç´¢æ¨¡ç‰ˆ
    logger.info("   ğŸ” æ­£åœ¨æœç´¢åŒ¹é…æ¨¡ç‰ˆ...")
    try:
        templates = await search_templates(
            query_text=search_query,
            top_k=5,
            threshold=0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
        )
    except Exception as e:
        logger.error(f"   âŒ æ¨¡ç‰ˆæœç´¢å¤±è´¥: {e}")
        execution = StepExecution(
            step=current_step,
            execution_res=f"æ¨¡ç‰ˆæœç´¢å¤±è´¥: {str(e)}",
            status="failed"
        )
        return {
            "pending_steps": remaining_steps,
            "completed_steps": [execution],
            "matched_template": None
        }

    # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…æ¨¡ç‰ˆ
    if not templates:
        logger.info("   ğŸ“­ æœªæ‰¾åˆ°åŒ¹é…çš„æ¨¡ç‰ˆ")
        execution = StepExecution(
            step=current_step,
            execution_res="æœªæ‰¾åˆ°åŒ¹é…çš„æ¨¡ç‰ˆï¼Œå°†ä»é›¶å¼€å§‹è§„åˆ’",
            status="completed"
        )
        logger.info("=" * 60)
        return {
            "pending_steps": remaining_steps,
            "completed_steps": [execution],
            "matched_template": None
        }

    # æ‰¾åˆ°æ¨¡ç‰ˆ
    logger.info(f"   âœ… æ‰¾åˆ° {len(templates)} ä¸ªåŒ¹é…æ¨¡ç‰ˆ:")
    for idx, t in enumerate(templates, 1):
        logger.info(f"      {idx}. {t['title']} (ç›¸ä¼¼åº¦: {t['similarity_score']:.2%})")

    # æ„å»ºæœç´¢ç»“æœæè¿°
    result_description = f"æ‰¾åˆ° {len(templates)} ä¸ªåŒ¹é…æ¨¡ç‰ˆ:\n"
    for idx, t in enumerate(templates, 1):
        sections_info = ""
        if t.get('sections'):
            sections_info = f", {len(t['sections'])} ä¸ªç« èŠ‚"
        result_description += f"\n{idx}. ã€{t['title']}ã€‘\n"
        result_description += f"   åˆ†ç±»: {t['category']}\n"
        result_description += f"   ç›¸ä¼¼åº¦: {t['similarity_score']:.2%}{sections_info}\n"
        result_description += f"   ç®€ä»‹: {t['summary'][:150]}...\n"

    execution = StepExecution(
        step=current_step,
        execution_res=result_description,
        status="completed"
    )

    # å¦‚æœåªæ‰¾åˆ°ä¸€ä¸ªé«˜ç›¸ä¼¼åº¦æ¨¡ç‰ˆï¼ˆ>0.8ï¼‰ï¼Œç›´æ¥è®¾ç½®ä¸º matched_template
    # å¦åˆ™è¿”å› Noneï¼Œè®© Planner å†³å®šæ˜¯å¦éœ€è¦ HUMAN_INVOLVEMENT è®©ç”¨æˆ·é€‰æ‹©
    matched_template = None
    if len(templates) == 1 and templates[0]['similarity_score'] > 0.8:
        matched_template = templates[0]
        logger.info(f"   ğŸ¯ é«˜ç›¸ä¼¼åº¦åŒ¹é…ï¼Œè‡ªåŠ¨é€‰æ‹©: {matched_template['title']}")
    else:
        logger.info(f"   ğŸ“‹ å¤šä¸ªæ¨¡ç‰ˆæˆ–ç›¸ä¼¼åº¦ä¸å¤Ÿé«˜ï¼Œéœ€è¦ç”¨æˆ·é€‰æ‹©")
        # å°†æ¨¡ç‰ˆåˆ—è¡¨å­˜å…¥æ‰§è¡Œç»“æœï¼Œä¾›åç»­ HUMAN_INVOLVEMENT ä½¿ç”¨
        execution.execution_res += f"\n\n[TEMPLATES_DATA]{json.dumps(templates, ensure_ascii=False)}"

    logger.success(f"âœ… [TEMPLATE SEARCH NODE] æœç´¢å®Œæˆ")
    logger.info(f"   ğŸ“‹ å‰©ä½™æ­¥éª¤: {len(remaining_steps)} ä¸ª")
    logger.info("=" * 60)

    return {
        "pending_steps": remaining_steps,
        "completed_steps": [execution],
        "matched_template": matched_template
    }


# ==================== åè°ƒå™¨å’Œè§„åˆ’å™¨èŠ‚ç‚¹ ====================

async def coordinator_step(state: PlanExecuteState) -> Command[Literal["planner", "__end__"]]:
    """
    åè°ƒå™¨èŠ‚ç‚¹ï¼šä¸å®¢æˆ·æ²Ÿé€šå¹¶æ ¹æ®è¯·æ±‚çš„æ¸…æ™°åº¦è·¯ç”±ä»»åŠ¡
    """
    messages = apply_prompt_template("coordinator", state)
    llm = init_chat_model("deepseek:deepseek-chat")
    response = await llm.with_structured_output(CoordinatorDecision).ainvoke(messages)

    messages = state.get("conversation_messages", [])
    if response.message:
        messages.append(AIMessage(content=response.message, name="Coordinator"))

    logger.info(f"ğŸ¯ [COORDINATOR] å†³ç­–: {response.next_action}")
    goto_node = response.goto if response.goto else "__end__"

    update_state = {
        "conversation_messages": messages,
        "language": response.language,
    }

    logger.info(f"   â¡ï¸  è·¯ç”±åˆ°: {goto_node}")
    return Command(goto=response.goto, update=update_state)


async def plan_step(state: PlanExecuteState):
    """
    è§„åˆ’å™¨èŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ç”Ÿæˆåˆæ­¥æ‰§è¡Œè®¡åˆ’
    """
    language_code = state.get('language', 'zh-CN')

    logger.info("ğŸ“‹ [PLANNER] å¼€å§‹ç”Ÿæˆæ‰§è¡Œè®¡åˆ’...")

    llm = init_chat_model("deepseek:deepseek-reasoner")
    messages = apply_prompt_template("planner/planner", state)
    planner = llm.with_structured_output(Plan)

    plan = await planner.ainvoke(messages)

    logger.success(f"âœ… [PLANNER] ç”Ÿæˆäº† {len(plan.steps)} ä¸ªæ‰§è¡Œæ­¥éª¤:")
    for idx, step in enumerate(plan.steps, start=1):
        logger.info(f"   {idx}. [{step.step_type.value.upper()}] {step.target}")

    update_state = {
        "conversation_messages": [AIMessage(content=plan.thought)],
        "pending_steps": plan.steps,
        "language": language_code
    }

    return update_state


# ==================== è·¯ç”±èŠ‚ç‚¹ ====================

async def route_step(state: PlanExecuteState):
    """
    è·¯ç”±èŠ‚ç‚¹ï¼šè®°å½•æ—¥å¿—å¹¶å‡†å¤‡è·¯ç”±
    è¿™æ˜¯ä¸€ä¸ªçœŸæ­£çš„èŠ‚ç‚¹å‡½æ•°ï¼Œå¿…é¡»è¿”å›å­—å…¸
    """
    pending_steps = state.get("pending_steps", [])

    if not pending_steps:
        logger.info("ğŸ [ROUTER] æ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ï¼Œå‡†å¤‡ç»“æŸå·¥ä½œæµ")
        return {}

    current_step = pending_steps[0]
    logger.info(f"ğŸ”€ [ROUTER] å½“å‰æ­¥éª¤ç±»å‹: {current_step.step_type.value}")
    logger.info(f"   ğŸ“ æ­¥éª¤ç›®æ ‡: {current_step.target}")

    return {}  # èŠ‚ç‚¹å‡½æ•°å¿…é¡»è¿”å›å­—å…¸


def _route_decision(state: PlanExecuteState) -> Literal[
    "execute_research", "execute_human_involvement", "execute_template_search", "execute_writing_blueprint", "__end__"]:
    """
    è·¯ç”±å†³ç­–å‡½æ•°ï¼šæ ¹æ®å½“å‰å¾…æ‰§è¡Œæ­¥éª¤çš„ç±»å‹å†³å®šè·¯ç”±
    è¿™æ˜¯æ¡ä»¶å‡½æ•°ï¼Œç”¨äº add_conditional_edgesï¼Œè¿”å›å­—ç¬¦ä¸²

    è¿”å›å€¼ï¼š
    - "execute_research": è·¯ç”±åˆ°ç ”ç©¶èŠ‚ç‚¹
    - "execute_human_involvement": è·¯ç”±åˆ°äººç±»å‚ä¸èŠ‚ç‚¹
    - "execute_template_search": è·¯ç”±åˆ°æ¨¡ç‰ˆæœç´¢èŠ‚ç‚¹
    - "execute_writing_blueprint": è·¯ç”±åˆ°è“å›¾æ„å»ºèŠ‚ç‚¹
    - "__end__": æ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ï¼Œç»“æŸå·¥ä½œæµ
    """
    pending_steps = state.get("pending_steps", [])

    if not pending_steps:
        return "__end__"

    current_step = pending_steps[0]
    step_type = current_step.step_type

    # æ ¹æ®æ­¥éª¤ç±»å‹è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹
    route_map = {
        StepType.RESEARCH: "execute_research",
        StepType.HUMAN_INVOLVEMENT: "execute_human_involvement",
        StepType.TEMPLATE_SEARCH: "execute_template_search",
        StepType.WRITING_BLUEPRINT: "execute_writing_blueprint",
    }

    next_node = route_map.get(step_type, "__end__")
    logger.info(f"   â¡ï¸  è·¯ç”±å†³ç­–: {next_node}")

    return next_node


# ==================== ä¸‰ä¸ªç‹¬ç«‹çš„æ‰§è¡ŒèŠ‚ç‚¹ ====================

async def execute_research_node(state: PlanExecuteState):
    """
    ğŸ”¬ ç ”ç©¶èŠ‚ç‚¹ï¼šæ‰§è¡Œç ”ç©¶ç±»å‹çš„æ­¥éª¤

    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ Research Subgraph è¿›è¡Œå¹¶è¡Œç ”ç©¶
    - æ”¯æŒå¤šä¸ªæœç´¢ä¸»é¢˜ï¼ˆactions ä¸º listï¼‰
    - æ›´æ–°å®Œæˆçš„æ­¥éª¤å’Œå¾…æ‰§è¡Œæ­¥éª¤
    """
    logger.info("=" * 60)
    logger.info("ğŸ”¬ [RESEARCH NODE] èŠ‚ç‚¹å¯åŠ¨")
    logger.info("=" * 60)

    pending_steps = state.get("pending_steps", [])

    if not pending_steps:
        logger.warning("âš ï¸  [RESEARCH NODE] æ²¡æœ‰å¾…æ‰§è¡Œçš„æ­¥éª¤")
        return {"pending_steps": []}

    # è·å–å½“å‰æ­¥éª¤
    current_step = pending_steps[0]
    remaining_steps = pending_steps[1:]

    logger.info(f"ğŸ“Œ [RESEARCH NODE] å½“å‰ä»»åŠ¡: {current_step.target}")

    # è°ƒç”¨ç ”ç©¶å­å›¾
    result = await _execute_research_logic(current_step, state)

    # åˆ›å»ºæ‰§è¡Œè®°å½•
    execution = StepExecution(
        step=current_step,
        execution_res=result,
        status="completed"
    )

    logger.success(f"âœ… [RESEARCH NODE] ç ”ç©¶å®Œæˆ")
    logger.info(f"   ğŸ“Š ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
    logger.info(f"   ğŸ“‹ å‰©ä½™æ­¥éª¤: {len(remaining_steps)} ä¸ª")
    logger.info("=" * 60)

    return {
        "pending_steps": remaining_steps,
        "completed_steps": [execution]
    }


async def execute_human_involvement_node(state: PlanExecuteState):
    """
    ğŸ‘¤ äººç±»å‚ä¸èŠ‚ç‚¹ï¼šéœ€è¦äººç±»è¾“å…¥çš„æ­¥éª¤

    åŠŸèƒ½ï¼š
    - ç”Ÿæˆä¸ç”¨æˆ·äº¤äº’çš„æ¶ˆæ¯
    - ä½¿ç”¨ interrupt ä¸­æ–­å·¥ä½œæµç­‰å¾…ç”¨æˆ·è¾“å…¥
    - æ”¶é›†ç”¨æˆ·åé¦ˆå¹¶è®°å½•
    """
    logger.info("=" * 60)
    logger.warning("ğŸ‘¤ [HUMAN NODE] èŠ‚ç‚¹å¯åŠ¨ - éœ€è¦äººç±»å‚ä¸")
    logger.info("=" * 60)

    pending_steps = state.get("pending_steps", [])

    if not pending_steps:
        logger.warning("âš ï¸  [HUMAN NODE] æ²¡æœ‰å¾…æ‰§è¡Œçš„æ­¥éª¤")
        return {"pending_steps": []}

    # è·å–å½“å‰æ­¥éª¤
    current_step = pending_steps[0]
    remaining_steps = pending_steps[1:]

    logger.info(f"ğŸ“Œ [HUMAN NODE] å½“å‰ä»»åŠ¡: {current_step.target}")
    logger.info(f"   ğŸ¯ è¡ŒåŠ¨è¦æ±‚: {current_step.actions}")

    # æ‰§è¡Œäººç±»å‚ä¸é€»è¾‘
    result = await _execute_human_involvement_logic(current_step, state)

    # åˆ›å»ºæ‰§è¡Œè®°å½•
    execution = StepExecution(
        step=current_step,
        execution_res=result,
        status="completed"
    )

    logger.success(f"âœ… [HUMAN NODE] ç”¨æˆ·åé¦ˆå·²æ”¶é›†")
    logger.info(f"   ğŸ“‹ å‰©ä½™æ­¥éª¤: {len(remaining_steps)} ä¸ª")
    logger.info("=" * 60)

    return {
        "pending_steps": remaining_steps,
        "completed_steps": [execution]
    }


async def execute_writing_blueprint_node(state: PlanExecuteState):
    """
    ğŸ“ è“å›¾æ„å»ºèŠ‚ç‚¹ï¼šæ„å»ºå†™ä½œè“å›¾

    åŠŸèƒ½ï¼š
    - æ”¶é›†æ‰€æœ‰å·²å®Œæˆçš„ç ”ç©¶ç»“æœ
    - åŸºäºç ”ç©¶å†…å®¹ç”Ÿæˆå†™ä½œè“å›¾
    - æ›´æ–° blueprint_draft å­—æ®µ
    """
    logger.info("=" * 60)
    logger.info("ğŸ“ [BLUEPRINT NODE] èŠ‚ç‚¹å¯åŠ¨")
    logger.info("=" * 60)

    pending_steps = state.get("pending_steps", [])

    if not pending_steps:
        logger.warning("âš ï¸  [BLUEPRINT NODE] æ²¡æœ‰å¾…æ‰§è¡Œçš„æ­¥éª¤")
        return {"pending_steps": []}

    # è·å–å½“å‰æ­¥éª¤
    current_step = pending_steps[0]
    remaining_steps = pending_steps[1:]

    logger.info(f"ğŸ“Œ [BLUEPRINT NODE] å½“å‰ä»»åŠ¡: {current_step.target}")

    # æ‰§è¡Œè“å›¾æ„å»ºé€»è¾‘
    result = await _execute_blueprint_logic(current_step, state)

    # åˆ›å»ºæ‰§è¡Œè®°å½•
    execution = StepExecution(
        step=current_step,
        execution_res=result,
        status="completed"
    )

    logger.success(f"âœ… [BLUEPRINT NODE] è“å›¾æ„å»ºå®Œæˆ")
    logger.info(f"   ğŸ“Š è“å›¾é•¿åº¦: {len(result)} å­—ç¬¦")
    logger.info(f"   ğŸ“‹ å‰©ä½™æ­¥éª¤: {len(remaining_steps)} ä¸ª")
    logger.info("=" * 60)

    return {
        "pending_steps": remaining_steps,
        "completed_steps": [execution],
        "blueprint_draft": result  # æ›´æ–°è“å›¾å­—æ®µ
    }


# ==================== æ‰§è¡Œé€»è¾‘è¾…åŠ©å‡½æ•°ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰====================

async def _execute_research_logic(step: Step, state: PlanExecuteState) -> str:
    """
    ç ”ç©¶æ‰§è¡Œé€»è¾‘ - ä½¿ç”¨ Research Subgraph
    """
    from app.agents.core.publisher.subgraphs.research.agent import run_research_subgraph

    logger.info("   ğŸ” [RESEARCH] å¼€å§‹è°ƒç”¨ç ”ç©¶å­å›¾")

    actions = step.actions if isinstance(step.actions, list) else [step.actions]
    logger.info(f"   ğŸ“š [RESEARCH] éœ€è¦ç ”ç©¶ {len(actions)} ä¸ªä¸»é¢˜:")
    for idx, topic in enumerate(actions, start=1):
        logger.info(f"      {idx}. {topic}")

    language_code = state.get('language', 'zh-CN')

    # è°ƒç”¨ research subgraph
    logger.info("   âš™ï¸  [RESEARCH] æ‰§è¡Œä¸­...")
    final_result = await run_research_subgraph(
        topics=actions,
        need_search=True,
        language=language_code
    )

    logger.info(f"   âœ“ [RESEARCH] ç ”ç©¶å­å›¾æ‰§è¡Œå®Œæˆ")
    return final_result


async def _execute_human_involvement_logic(step: Step, state: PlanExecuteState) -> str:
    """
    äººç±»å‚ä¸æ‰§è¡Œé€»è¾‘ - ç”Ÿæˆæç¤ºå¹¶ä¸­æ–­ç­‰å¾…ç”¨æˆ·è¾“å…¥
    """
    logger.info("   ğŸ’¬ [HUMAN] å‡†å¤‡ç”Ÿæˆç”¨æˆ·äº¤äº’æ¶ˆæ¯")

    lang = state.get("language")
    language_code = state.get('language', 'zh-CN')
    completed_steps = state.get("completed_steps", [])

    # æ„å»ºå·²å®Œæˆæ­¥éª¤çš„æ‘˜è¦
    completed_steps_json = json.dumps(
        [step.model_dump() for step in completed_steps],
        indent=2,
        ensure_ascii=False
    )

    task_formatted = f"""
    You are an AI assistant guiding a user through a multi-step task.
    The current step target is "{step.target}" with action: "{step.actions}".
    Previous completed steps are: {completed_steps_json}.
    - Based on the completed steps and the current step's actions, generate a concise, engaging message to interact with the user. 
    - This message should initiate or continue the discussion as outlined in the actions, prompting user input where needed to fulfill the step's goal. 
    - Keep the tone collaborative and focused on the task. Output only the message, nothing else.
    - Always use the language specified by the language {language_code}
    """

    logger.info("   ğŸ¤– [HUMAN] è°ƒç”¨ LLM ç”Ÿæˆäº¤äº’æ¶ˆæ¯")
    llm = init_chat_model("deepseek:deepseek-chat")
    response = await llm.ainvoke(task_formatted)

    # é€šè¿‡ LangGraph çš„ interrupt æœºåˆ¶ä¸­æ–­ç­‰å¾…ç”¨æˆ·è¾“å…¥
    interrupt_payload = {
        "type": "human_involvement",
        "target": step.target,
        "actions": step.actions,
        "message": response.content,
        "language": lang,
    }

    logger.info(f"   â¸ï¸  [HUMAN] ä¸­æ–­å·¥ä½œæµï¼Œç­‰å¾…ç”¨æˆ·åé¦ˆ")
    logger.debug(f"   ğŸ’­ [HUMAN] æç¤ºæ¶ˆæ¯: {response.content[:100]}...")

    user_feedback = interrupt(interrupt_payload)

    feedback_str = str(user_feedback)
    if len(feedback_str) > 100:
        logger.success(f"   âœ“ [HUMAN] æ”¶åˆ°ç”¨æˆ·åé¦ˆ: {feedback_str[:100]}...")
    else:
        logger.success(f"   âœ“ [HUMAN] æ”¶åˆ°ç”¨æˆ·åé¦ˆ: {feedback_str}")

    # è¿”å›ç»“æ„åŒ–æ–‡æœ¬
    return f"[HUMAN FEEDBACK]\nTask: {step.target}\nFeedback: {user_feedback}"


async def _execute_blueprint_logic(step: Step, state: PlanExecuteState) -> str:
    """
    è“å›¾æ„å»ºæ‰§è¡Œé€»è¾‘ - åŸºäºç ”ç©¶ç»“æœç”Ÿæˆå†™ä½œè“å›¾
    """
    logger.info("   ğŸ“– [BLUEPRINT] æ”¶é›†ç ”ç©¶ç»“æœ")
    language_code = state.get('language', 'zh-CN')

    # æ”¶é›†ä¹‹å‰æ‰€æœ‰ç ”ç©¶æ­¥éª¤çš„ç»“æœ
    past_research = []
    for execution in state.get("completed_steps", []):
        if execution.step.step_type == StepType.RESEARCH:
            past_research.append(f"{execution.step.target}:\n{execution.execution_res}")
            logger.info(f"      âœ“ å¼•ç”¨ç ”ç©¶: {execution.step.target}")

    context = "\n\n".join(past_research) if past_research else "No previous research available."
    logger.info(f"   ğŸ“š [BLUEPRINT] åŸºäº {len(past_research)} ä¸ªç ”ç©¶ç»“æœæ„å»ºè“å›¾")

    task_formatted = f"""
    Based on the following research and information, create a detailed writing blueprint:

    Previous Research:
    {context}

    Task: {step.target}
    Requirements: {step.actions}

    Please create a comprehensive writing blueprint including:
    1. Main structure and outline
    2. Key points to cover
    3. Supporting arguments and evidence
    4. Recommended writing style and tone

    Language: {language_code}
    """

    logger.info("   ğŸ¤– [BLUEPRINT] è°ƒç”¨ LLM ç”Ÿæˆè“å›¾")
    llm = init_chat_model("deepseek:deepseek-chat")
    response = await llm.ainvoke(task_formatted)

    logger.info(f"   âœ“ [BLUEPRINT] è“å›¾ç”Ÿæˆå®Œæˆ")
    return response.content


# ==================== é‡æ–°è§„åˆ’èŠ‚ç‚¹ ====================

async def replan_step(state: PlanExecuteState):
    """
    é‡æ–°è§„åˆ’èŠ‚ç‚¹ï¼šæ ¹æ®å·²æ‰§è¡Œçš„æ­¥éª¤å’Œå‰©ä½™ä»»åŠ¡ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
    """
    logger.info("=" * 60)
    logger.info("ğŸ”„ [REPLAN] é‡æ–°è§„åˆ’èŠ‚ç‚¹å¯åŠ¨")
    logger.info("=" * 60)

    # ä»çŠ¶æ€ä¸­è·å–æ•°æ®
    completed_steps = state.get("completed_steps", [])
    pending_steps = state.get("pending_steps", [])
    conversation_messages = state.get("conversation_messages", [])

    logger.info(f"   ğŸ“Š [REPLAN] å·²å®Œæˆ: {len(completed_steps)} ä¸ªæ­¥éª¤")
    logger.info(f"   ğŸ“‹ [REPLAN] å¾…æ‰§è¡Œ: {len(pending_steps)} ä¸ªæ­¥éª¤")

    # æ„å»ºå·²å®Œæˆæ­¥éª¤æ‘˜è¦
    completed_summary = []
    for execution in completed_steps:
        step = execution.step
        result = execution.execution_res
        result_preview = result[:10000] + "..." if len(result) > 10000 else result

        completed_summary.append({
            "step_number": len(completed_summary) + 1,
            "step_type": step.step_type.value,
            "target": step.target,
            "result_preview": result_preview,
            "status": execution.status,
        })

    # æ„å»ºå¾…æ‰§è¡Œæ­¥éª¤æ‘˜è¦
    pending_summary = []
    for idx, step in enumerate(pending_steps):
        pending_summary.append({
            "step_number": len(completed_steps) + idx + 1,
            "step_type": step.step_type.value,
            "target": step.target,
            "actions": step.actions
        })

    # è°ƒç”¨ LLM è¿›è¡Œé‡æ–°è§„åˆ’
    logger.info("   ğŸ¤– [REPLAN] è°ƒç”¨ LLM åˆ†æå½“å‰çŠ¶æ€")

    llm = init_chat_model("deepseek:deepseek-chat")
    replanner = replanner_prompt | llm.with_structured_output(ReplanSteps)

    prompt_input = {
        "conversation_messages": conversation_messages,
        "completed_steps": completed_summary,
        "pending_steps": pending_summary,
        "total_completed": len(completed_steps),
        "total_pending": len(pending_steps),
    }

    output = await replanner.ainvoke(prompt_input)

    logger.info(f"   ğŸ¯ [REPLAN] è§„åˆ’ç»“æœ: {len(output.steps)} ä¸ªæ–°æ­¥éª¤")
    logger.info(f"   ğŸ’­ [REPLAN] æ¨ç†: {output.reasoning[:200]}...")

    if not output.steps:
        # é¡¹ç›®å®Œæˆ
        logger.success("ğŸŠ [REPLAN] é¡¹ç›®å®Œæˆï¼Œå‡†å¤‡ç»“æŸå·¥ä½œæµ")
        logger.info("=" * 60)

        return {
            "response": output.reasoning,
            "pending_steps": []
        }
    else:
        # ç»§ç»­æ‰§è¡Œ
        logger.info(f"   ğŸ“‹ [REPLAN] æ›´æ–°æ‰§è¡Œè®¡åˆ’:")
        for idx, step in enumerate(output.steps, start=1):
            logger.info(f"      {idx}. [{step.step_type.value.upper()}] {step.target}")
        logger.info("=" * 60)

        return {
            "pending_steps": output.steps
        }


# ==================== æ¡ä»¶åˆ¤æ–­å‡½æ•° ====================

def should_replan(state: PlanExecuteState) -> Literal["route_step", "replan", "END"]:
    """
    æ¡ä»¶åˆ¤æ–­ï¼šæ‰§è¡ŒèŠ‚ç‚¹å®Œæˆåï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨

    é€»è¾‘ï¼š
    1. å¦‚æœæ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ -> ç»“æŸ
    2. å¦‚æœåˆšå®Œæˆçš„æ˜¯ HUMAN_INVOLVEMENT -> é‡æ–°è§„åˆ’
    3. å…¶ä»–æƒ…å†µ -> ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªæ­¥éª¤
    """
    pending_steps = state.get("pending_steps", [])

    # æ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ï¼Œç»“æŸå·¥ä½œæµ
    if not pending_steps:
        logger.info("ğŸ [SHOULD_REPLAN] æ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ï¼Œå‡†å¤‡ç»“æŸ")
        return END

    # æ£€æŸ¥åˆšå®Œæˆçš„æ­¥éª¤ç±»å‹
    completed = state.get("completed_steps", [])
    if completed:
        last_step_type = completed[-1].step.step_type

        # å¦‚æœæ˜¯äººç±»å‚ä¸ï¼Œéœ€è¦é‡æ–°è§„åˆ’
        if last_step_type == StepType.HUMAN_INVOLVEMENT:
            logger.info(f"ğŸ”„ [SHOULD_REPLAN] æ£€æµ‹åˆ° {last_step_type.value}ï¼Œè§¦å‘é‡æ–°è§„åˆ’")
            return "replan"

    # å…¶ä»–æƒ…å†µç»§ç»­æ‰§è¡Œ
    logger.info("â¡ï¸  [SHOULD_REPLAN] ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªæ­¥éª¤")
    return "route_step"


def should_end(state: PlanExecuteState) -> Literal["route_step", "END"]:
    """
    æ¡ä»¶åˆ¤æ–­ï¼šé‡æ–°è§„åˆ’åï¼Œå†³å®šæ˜¯ç»§ç»­æ‰§è¡Œè¿˜æ˜¯ç»“æŸ

    é€»è¾‘ï¼š
    1. å¦‚æœæœ‰ responseï¼ˆé¡¹ç›®å®Œæˆæ ‡å¿—ï¼‰-> ç»“æŸ
    2. å¦‚æœæ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ -> ç»“æŸ
    3. å…¶ä»–æƒ…å†µ -> ç»§ç»­æ‰§è¡Œ
    """
    has_response = bool(state.get("response"))
    has_pending = bool(state.get("pending_steps"))

    if has_response:
        logger.info("ğŸ [SHOULD_END] æ£€æµ‹åˆ°æœ€ç»ˆå“åº”ï¼Œå·¥ä½œæµå°†ç»“æŸ")
        return END
    elif not has_pending:
        logger.warning("âš ï¸  [SHOULD_END] æ²¡æœ‰å¾…æ‰§è¡Œæ­¥éª¤ä¸”æ²¡æœ‰å“åº”ï¼Œå¼‚å¸¸æƒ…å†µï¼Œç»“æŸå·¥ä½œæµ")
        return END
    else:
        logger.info("â¡ï¸  [SHOULD_END] ç»§ç»­æ‰§è¡Œæ–°çš„æ­¥éª¤")
        return "route_step"