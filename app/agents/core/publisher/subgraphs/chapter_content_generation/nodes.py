"""
Chapter Content Generation Nodes - Iterative Chapter Generation Implementation
"""
import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from loguru import logger
from typing import Dict, Any, List
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from pydantic import BaseModel, Field
from langgraph.types import Send, Command

from app.agents.core.publisher.subgraphs.chapter_content_generation.state import (
    ChapterIterativeState,
)
from app.agents.prompts.template import render_prompt_template
from app.agents.tools.generation.chart_generation import generate_chart
from app.agents.tools.thinking.thinking_tools import think, criticize

load_dotenv()

# Prompt æ¨¡æ¿è·¯å¾„å‰ç¼€
PROMPT_PATH = "publisher_prompts/chapter_content_generation"

# æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
MAX_ITERATIONS = 3


# ============================================================
# Pydantic Schema for Structured Output
# ============================================================

class QueryList(BaseModel):
    """List of search queries"""
    queries: List[str] = Field(description="List of search queries (1-3)")


class DraftEvaluation(BaseModel):
    """Draft evaluation result"""
    is_satisfied: bool = Field(description="Whether the current draft is satisfactory (True if coverage >= 0.7)")
    coverage_score: float = Field(description="Coverage score (0-1)")
    follow_up_queries: List[str] = Field(default=[], description="Follow-up search queries to fill gaps. Empty list if satisfied.")


# ============================================================
# Node 1: Generate Initial Queries (åªåœ¨ç¬¬ä¸€è½®æ‰§è¡Œ)
# ============================================================

def generate_queries_node(state: ChapterIterativeState) -> Command:
    """
    Node 1: Generate initial search queries based on chapter outline

    åªåœ¨ç¬¬ä¸€è½®æ‰§è¡Œï¼Œæ ¹æ® chapter_outline ç”Ÿæˆ 3 ä¸ªåˆå§‹ queries

    Output: Command with Send list to search_node
    """
    chapter_id = state["chapter_id"]
    chapter_outline = state["chapter_outline"]
    chapter_title = chapter_outline.title
    chapter_description = getattr(chapter_outline, "description", "")
    content_requirements = getattr(chapter_outline, "content_requirements", "")
    writing_guidance = getattr(chapter_outline, "writing_guidance", "")

    llm = ChatDeepSeek(model="deepseek-chat", temperature=0)
    llm_with_structure = llm.with_structured_output(QueryList)

    logger.info(f"  ğŸ” [Chapter {chapter_id}] Generating initial search queries...")
    prompt = render_prompt_template(f"{PROMPT_PATH}/generate_queries_initial", {
        "chapter_title": chapter_title,
        "chapter_description": chapter_description,
        "content_requirements": content_requirements,
        "writing_guidance": writing_guidance,
    })

    try:
        result = llm_with_structure.invoke(prompt)
        queries = result.queries[:3]  # Ensure max 3 queries

        logger.info(f"    âœ“ Generated {len(queries)} queries:")
        for i, query in enumerate(queries, 1):
            logger.info(f"      {i}. {query}")

        # Build Send list for parallel search
        send_list = [
            Send("search", {
                "chapter_id": chapter_id,
                "chapter_outline": chapter_outline,
                "search_query": query,
            })
            for query in queries
        ]

        logger.info(f"    âœ“ Dispatching {len(send_list)} parallel searches...")

        return Command(goto=send_list)

    except Exception as e:
        logger.error(f"    âš ï¸  Query generation failed: {e}")
        fallback_query = f"{chapter_title} {content_requirements}"
        logger.info(f"    â†³ Using fallback query: {fallback_query}")

        return Command(goto=[Send("search", {
            "chapter_id": chapter_id,
            "chapter_outline": chapter_outline,
            "search_query": fallback_query,
        })])


# ============================================================
# Node 2: Search (Pure Search Node)
# ============================================================

def search_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Node 2: Pure search execution (supports parallel execution)

    Input: search_query (passed via Send)
    Output: search_results (list with single result, aggregated by operator.add)
    """
    chapter_id = state.get("chapter_id", "?")
    search_query = state.get("search_query", "")

    logger.info(f"  ğŸ” [Chapter {chapter_id}] Searching...")
    logger.info(f"    Query: {search_query}")

    try:
        from app.agents.tools.search.tavily_search import searcher
    except ImportError:
        logger.error("    âš ï¸  Cannot import search tool")
        return {
            "search_results": [{
                "query": search_query,
                "content": f"Search tool unavailable. Placeholder content for: {search_query}"
            }]
        }

    try:
        search_content = searcher.invoke(search_query, max_results=5)
        logger.info(f"    âœ“ Search completed ({len(search_content)} characters)")

        return {
            "search_results": [{
                "query": search_query,
                "content": search_content
            }]
        }

    except Exception as e:
        logger.error(f"    âš ï¸  Search failed: {e}")
        return {
            "search_results": [{
                "query": search_query,
                "content": f"Search failed: {str(e)}. Query was: {search_query}"
            }]
        }


# ============================================================
# Node 3: Write/Refine Draft (Agent with Chart Generation)
# ============================================================

async def write_node(state: ChapterIterativeState) -> Dict[str, Any]:
    """
    Node 3: Write or refine draft using Agent with chart generation capability

    Input: search_results (list), draft, iteration, chapter_outline, writer context
    Output: updated draft, iteration+1, search_results=[] (æ¸…ç©ºä¸ºä¸‹è½®å‡†å¤‡)
    """
    chapter_id = state["chapter_id"]
    chapter_outline = state["chapter_outline"]
    chapter_title = chapter_outline.title
    iteration = state.get("iteration", 0)
    draft = state.get("draft", "")
    search_results_list = state.get("search_results", [])
    target_word_count = getattr(chapter_outline, "estimated_words", 800)

    description = getattr(chapter_outline, "description", "")
    content_requirements = getattr(chapter_outline, "content_requirements", "")
    writing_guidance = getattr(chapter_outline, "writing_guidance", "")
    visual_elements = getattr(chapter_outline, "visual_elements", False)

    # è·å–å†™ä½œä¸Šä¸‹æ–‡ï¼ˆä» section_writer ä¼ å…¥ï¼‰
    document_outline = state.get("document_outline")
    writer_role = state.get("writer_role", "Expert Content Writer")
    writer_profile = state.get("writer_profile", "")
    writing_principles = state.get("writing_principles", [])

    # ä» document_outline è·å–å†™ä½œé£æ ¼ä¿¡æ¯
    writing_tone = getattr(document_outline, "writing_tone", "") if document_outline else ""
    writing_style = getattr(document_outline, "writing_style", "") if document_outline else ""
    language = getattr(document_outline, "language", "Chinese") if document_outline else "Chinese"

    # ==================== ç« èŠ‚å†™ä½œæ—¥å¿— ====================
    logger.info("")
    logger.info("=" * 80)
    logger.info(f"ğŸ“ [WRITE_NODE] Chapter {chapter_id}: {chapter_title}")
    logger.info("=" * 80)
    logger.info(f"  ğŸ“‹ Iteration: {iteration + 1}")
    logger.info(f"  ğŸ‘¤ Writer: {writer_role}")
    logger.info(f"  ğŸ“Š Chart generation: {'âœ… Enabled' if visual_elements else 'âŒ Disabled'}")
    logger.info(f"  ğŸ¯ Target words: {target_word_count}")

    # Initialize LLM
    llm = init_chat_model("deepseek:deepseek-chat")

    # Format search results
    search_results_text = "\n\n---\n\n".join([
        f"**Query**: {r['query']}\n**Results**:\n{r['content']}"
        for r in search_results_list
    ]) if search_results_list else "No search results available."

    # è®¡ç®—è¿­ä»£çŠ¶æ€
    revision_needed = iteration > 0

    # æ„å»ºç»Ÿä¸€çš„æ¨¡æ¿å‚æ•°ï¼ˆåŒ…å«æ‰€æœ‰æ¨¡æ¿å¯èƒ½éœ€è¦çš„å˜é‡ï¼‰
    template_params = {
        # åŸºç¡€ state å­—æ®µ
        "chapter_id": chapter_id,
        "chapter_outline": chapter_outline,
        "document_outline": document_outline,

        # å†™ä½œä¸Šä¸‹æ–‡
        "writer_role": writer_role,
        "writer_profile": writer_profile,
        "writing_principles": writing_principles,
        "writing_tone": writing_tone,
        "writing_style": writing_style,
        "language": language,

        # å†…å®¹å‚æ•°
        "target_word_count": target_word_count,
        "search_results_text": search_results_text,
        "visual_elements": visual_elements,

        # è¿­ä»£æ§åˆ¶å‚æ•°
        "revision_needed": revision_needed,
        "draft": draft,  # å§‹ç»ˆä¼ é€’ï¼Œæ¨¡æ¿æ ¹æ® revision_needed å†³å®šæ˜¯å¦ä½¿ç”¨
    }

    # ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªç³»ç»Ÿæç¤ºå’Œä¸€ä¸ªç”¨æˆ·æç¤ºæ¨¡æ¿
    system_prompt = render_prompt_template(f"{PROMPT_PATH}/write_draft_system", template_params)
    user_prompt = render_prompt_template(f"{PROMPT_PATH}/write_draft_initial", template_params)

    # Create Agent with tools
    agent = create_agent(model=llm, tools=[generate_chart], system_prompt=system_prompt)

    try:
        logger.info("-" * 80)
        logger.info(f"ğŸ“¤ [PROMPT] Chapter {chapter_id}: {chapter_title}")
        logger.info("-" * 80)
        logger.info(f"  ğŸ”§ System Prompt ({len(system_prompt)} chars):")
        # åªæ‰“å°å‰500å­—ç¬¦ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
        system_preview = system_prompt[:500] + "..." if len(system_prompt) > 500 else system_prompt
        for line in system_preview.split('\n')[:10]:  # åªæ˜¾ç¤ºå‰10è¡Œ
            logger.info(f"    | {line}")
        if len(system_prompt) > 500:
            logger.info(f"    | ... (truncated, total {len(system_prompt)} chars)")
        logger.info("")
        logger.info(f"  ğŸ“ User Prompt ({len(user_prompt)} chars):")
        user_preview = user_prompt[:500] + "..." if len(user_prompt) > 500 else user_prompt
        for line in user_preview.split('\n')[:10]:  # åªæ˜¾ç¤ºå‰10è¡Œ
            logger.info(f"    | {line}")
        if len(user_prompt) > 500:
            logger.info(f"    | ... (truncated, total {len(user_prompt)} chars)")
        logger.info("-" * 80)
        logger.info(f"ğŸš€ Starting Agent for Chapter {chapter_id}...")

        response = await agent.ainvoke(
            {"messages": [{"role": "user", "content": user_prompt}]},
            config={"recursion_limit": 50}
        )

        # Extract final content (last AI message)
        ai_messages = [m for m in response["messages"] if isinstance(m, AIMessage) and m.content]

        if ai_messages:
            new_draft = "\n\n".join(m.content.strip() for m in ai_messages)
        else:
            raise ValueError("Agent did not return valid content")


        logger.info("-" * 80)
        logger.info(f"ğŸ“¥ [RESULT] Chapter {chapter_id}: {chapter_title}")
        logger.info("-" * 80)
        logger.info(f"  âœ… Draft length: {len(new_draft)} characters")
        logger.info(f"  ğŸ“Š Charts generated: {new_draft.count('![')}")
        # æ˜¾ç¤ºç»“æœé¢„è§ˆ
        logger.info(f"  ğŸ“„ Content preview:")
        draft_preview = new_draft[:300] + "..." if len(new_draft) > 300 else new_draft
        for line in draft_preview.split('\n')[:8]:  # åªæ˜¾ç¤ºå‰8è¡Œ
            logger.info(f"    | {line[:80]}")  # æ¯è¡Œæœ€å¤š80å­—ç¬¦
        if len(new_draft) > 300:
            logger.info(f"    | ... (truncated, total {len(new_draft)} chars)")
        logger.info("=" * 80)
        logger.info("")

        # å®Œæ•´å†…å®¹è¾“å‡ºï¼ˆç”¨äºè°ƒè¯•ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ LOG_FULL_DRAFT=true æ§åˆ¶ï¼‰
        if os.getenv("LOG_FULL_DRAFT", "false").lower() == "true":
            logger.debug(f"[FULL_DRAFT] Chapter {chapter_id}: {chapter_title}\n{new_draft}")

        return {
            "draft": new_draft,
            "iteration": iteration + 1,
            "search_results": None  # ä½¿ç”¨ None è§¦å‘ reducer æ¸…ç©ºé€»è¾‘
        }

    except Exception as e:
        logger.error("")
        logger.error("=" * 80)
        logger.error(f"âŒ [ERROR] Chapter {chapter_id}: {chapter_title}")
        logger.error("=" * 80)
        logger.error(f"  âš ï¸ Writing failed: {e}")
        import traceback
        traceback.print_exc()
        fallback_draft = draft if draft else f"# {chapter_title}\n\nWriting failed: {str(e)}"
        return {
            "draft": fallback_draft,
            "iteration": iteration + 1,
            "search_results": None  # ä½¿ç”¨ None è§¦å‘ reducer æ¸…ç©ºé€»è¾‘
        }


# ============================================================
# Node 4: Evaluate Draft Quality
# ============================================================

def evaluate_node(state: ChapterIterativeState) -> Dict[str, Any]:
    """
    Node 4: Evaluate draft quality and generate follow-up queries if needed

    Input: draft, chapter_outline
    Output: is_satisfied, follow_up_queries (ç›´æ¥ç”Ÿæˆåç»­æŸ¥è¯¢)
    """
    chapter_id = state["chapter_id"]
    chapter_outline = state["chapter_outline"]
    chapter_title = chapter_outline.title
    draft = state.get("draft", "")
    target_word_count = getattr(chapter_outline, "estimated_words", 800)

    chapter_description = getattr(chapter_outline, "description", "")
    writing_guidance = getattr(chapter_outline, "writing_guidance", "")
    content_requirements = getattr(chapter_outline, "content_requirements", "")

    logger.info(f"  ğŸ“Š [Chapter {chapter_id}] Evaluating draft...")

    llm = init_chat_model("deepseek:deepseek-chat")
    llm_with_structure = llm.with_structured_output(DraftEvaluation)

    eval_prompt = render_prompt_template(f"{PROMPT_PATH}/evaluate_draft", {
        "chapter_title": chapter_title,
        "chapter_description": chapter_description,
        "writing_guidance": writing_guidance,
        "content_requirements": content_requirements,
        "target_word_count": target_word_count,
        "draft": draft,
    })

    try:
        result = llm_with_structure.invoke(eval_prompt)

        logger.info(f"    âœ“ Coverage score: {result.coverage_score:.2f}")
        logger.info(f"    âœ“ Satisfied: {'Yes' if result.is_satisfied else 'No'}")

        if not result.is_satisfied and result.follow_up_queries:
            logger.info(f"    â†³ Follow-up queries: {len(result.follow_up_queries)}")
            for i, q in enumerate(result.follow_up_queries, 1):
                logger.info(f"      {i}. {q}")

        return {
            "is_satisfied": result.is_satisfied,
            "follow_up_queries": result.follow_up_queries if not result.is_satisfied else []
        }

    except Exception as e:
        logger.error(f"    âš ï¸  Evaluation failed: {e}")
        draft_length = len(draft)
        is_satisfied = draft_length >= target_word_count * 0.8
        logger.info(f"    â†³ Fallback evaluation: draft length {draft_length} characters")

        return {
            "is_satisfied": is_satisfied,
            "follow_up_queries": []
        }


# ============================================================
# Conditional Edge Function (returns Send list or "finalize")
# ============================================================

def route_after_evaluate(state: ChapterIterativeState):
    """
    Conditional edge: determine whether to continue or finalize

    Rules:
    1. If iteration >= MAX_ITERATIONS â†’ "finalize" (å¼ºåˆ¶ç»ˆæ­¢)
    2. If is_satisfied=True OR follow_up_queries is empty â†’ "finalize"
    3. If is_satisfied=False AND follow_up_queries has items â†’ Send to search

    Returns: "finalize" or List[Send]
    """
    chapter_id = state["chapter_id"]
    chapter_outline = state["chapter_outline"]
    iteration = state.get("iteration", 0)
    is_satisfied = state.get("is_satisfied", False)
    follow_up_queries = state.get("follow_up_queries", [])

    logger.info(f"  ğŸ”€ [Chapter {chapter_id}] Decision point...")
    logger.info(f"    â†³ Iteration: {iteration}/{MAX_ITERATIONS}")
    logger.info(f"    â†³ Satisfied: {is_satisfied}")
    logger.info(f"    â†³ Follow-up queries: {len(follow_up_queries)}")

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    if iteration >= MAX_ITERATIONS:
        logger.warning(f"    âš ï¸  Max iterations ({MAX_ITERATIONS}) reached, forcing finalize")
        return "finalize"

    # Finalize if satisfied or no follow-up queries
    if is_satisfied or not follow_up_queries:
        logger.success(f"    âœ“ Finalizing chapter")
        return "finalize"

    # Send follow-up queries to search
    logger.info(f"    â†³ Dispatching {len(follow_up_queries)} follow-up searches...")
    return [
        Send("search", {
            "chapter_id": chapter_id,
            "chapter_outline": chapter_outline,
            "search_query": query,
        })
        for query in follow_up_queries
    ]


# ============================================================
# Final Node
# ============================================================

def finalize_node(state: ChapterIterativeState) -> Dict[str, Any]:
    """
    Final node: prepare output
    """
    chapter_id = state["chapter_id"]
    draft = state.get("draft", "")
    iteration = state.get("iteration", 0)

    logger.success(f"  ğŸ‰ [Chapter {chapter_id}] Content generation completed (Total iterations: {iteration})")

    return {
        "final_content": draft
    }
