# -*- coding: utf-8 -*-
"""
@File    :   chapter_reviewer.py
@Time    :   2025/11/14 10:14
@Author  :   pygao
@Version :   1.0
@Contact :   pygao.1@outlook.com
@License :   (C)Copyright 2025, GienTech Technology Co.,Ltd. All rights reserved.
@Desc    :   æ–‡ä»¶æè¿°
"""
from typing import Dict, Any
from loguru import logger
from app.agents.core.publisher.subgraphs.chapter_writing.state import ChapterState
from app.agents.schemas.review_schema import ChapterReviewResult
from langchain.chat_models import init_chat_model
from app.agents.prompts.template import apply_prompt_template, render_prompt_template


async def chapter_reviewer(state: ChapterState) -> Dict[str, Any]:
    """
    ç« èŠ‚å®¡æŸ¥èŠ‚ç‚¹ - ä½¿ç”¨ Structured Output

    èŒè´£ï¼š
    1. å¤šç»´åº¦è¯„åˆ†ç« èŠ‚è´¨é‡
    2. LLM å†³ç­–æ˜¯å¦éœ€è¦ä¿®è®¢
    3. ç”Ÿæˆä¿®è®¢æŒ‡ä»¤
    """
    from app.agents.schemas.review_schema import DimensionScore, Issue

    chapter_id = state["chapter_id"]
    chapter_title = state["chapter_outline"].title  # âœ… Bug #1: ä¿®å¤å­—æ®µè®¿é—®

    logger.info(f"  ğŸ“Š [Chapter {chapter_id} : title {chapter_title}] Reviewer: å¼€å§‹å®¡æŸ¥...")

    llm = init_chat_model("deepseek:deepseek-chat")

    # === 1. LLM è¯„å®¡ï¼ˆä½¿ç”¨ Structured Outputï¼‰===
    try:
        llm_with_structure = llm.with_structured_output(ChapterReviewResult)

        # ä½¿ç”¨ apply_prompt_template
        messages = apply_prompt_template(
            "chapter_writing/chapter_reviewer",
            {
                "chapter_id": chapter_id,
                "chapter_outline": state["chapter_outline"],
                "document_outline": state["document_outline"],
                "draft_content": state["draft_content"],
                "target_word_count": state["target_word_count"],
                "word_count": state["word_count"],
                "revision_count": state.get("revision_count", 0),
            }
        )
        print(messages)

        # âœ… Bug #2: ä¿®å¤å˜é‡åï¼ˆä½¿ç”¨ messages è€Œé review_promptï¼‰
        review_result = await llm_with_structure.ainvoke(messages)

        # âœ… Bug #3: ä¿®å¤å­—æ®µåï¼ˆoverall_score è€Œé scoreï¼‰
        logger.info(f"    â†³ æ€»åˆ†: {review_result.overall_score}/100\n")

    except Exception as e:
        logger.error(f"    âŒ å®¡æŸ¥å¤±è´¥: {str(e)}\n", exc_info=True)

        # è¿”å›é»˜è®¤å®¡æŸ¥ç»“æœ
        review_result = ChapterReviewResult(
            overall_score=50,
            dimensions={
                "content_coverage": DimensionScore(score=50, assessment="fair"),
                "content_depth": DimensionScore(score=50, assessment="fair"),
                "structure_logic": DimensionScore(score=50, assessment="fair"),
                "language_quality": DimensionScore(score=50, assessment="fair"),
                "format": DimensionScore(score=50, assessment="fair"),
                "length": DimensionScore(score=50, assessment="fair"),
            },
            issues=[
                Issue(
                    dimension="content_coverage",
                    severity="critical",
                    location="Entire chapter",
                    problem=f"Review process failed: {str(e)}",
                    suggestion="Check chapter content format or contact administrator"
                )
            ],
            summary="Review failed. Returning default result. Please check chapter content."
        )

    # === 2. è¿”å›è¯„å®¡ç»“æœ ===
    # æ³¨æ„ï¼šå†³ç­–é€»è¾‘å·²ç§»è‡³ agent.py çš„è·¯ç”±å‡½æ•°ä¸­
    # Reviewer åªè´Ÿè´£è¯„åˆ†å’Œå‘ç°é—®é¢˜ï¼Œä¸è´Ÿè´£å†³å®šæ˜¯å¦ä¿®è®¢

    # âœ… ä¿®å¤ï¼šæ¯æ¬¡å®¡æŸ¥åéƒ½å¢åŠ  revision_count
    # è¿™æ · decide_after_review å‡½æ•°å°±èƒ½çœ‹åˆ°æ­£ç¡®çš„ä¿®è®¢æ¬¡æ•°
    current_revision_count = state.get("revision_count", 0)
    new_revision_count = current_revision_count + 1

    logger.info(f"    â†³ ä¿®è®¢è®¡æ•°: {current_revision_count} â†’ {new_revision_count}")

    return {
        "revision_history": [review_result],
        "review_result": review_result,
        "revision_count": new_revision_count,  # âœ… æ¯æ¬¡å®¡æŸ¥åå¢åŠ 
    }