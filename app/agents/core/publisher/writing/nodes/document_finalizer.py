# -*- coding: utf-8 -*-
"""
Document Finalizer - æ–‡æ¡£å…ƒæ•°æ®æå–èŠ‚ç‚¹

èŒè´£ï¼š
1. ä½¿ç”¨ with_structured_output ä»å®Œæ•´æ–‡æ¡£ä¸­æå–å…ƒæ•°æ®
2. ç»Ÿè®¡æ–‡æ¡£å­—æ•°ï¼ˆæ’é™¤å›¾è¡¨ã€å¼•ç”¨ç­‰ï¼‰
3. ç”Ÿæˆ descriptionã€categoryã€tags ç­‰ä¿¡æ¯
"""
import re
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from loguru import logger
from langchain.chat_models import init_chat_model
from app.agents.core.publisher.writing.state import DocumentState
from app.agents.prompts.template import render_prompt_template


class DocumentMetadata(BaseModel):
    """æ–‡æ¡£å…ƒæ•°æ®ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹"""

    title: str = Field(
        description="æ–‡æ¡£æ ‡é¢˜ï¼Œç®€æ´æ˜äº†"
    )
    description: str = Field(
        description="æ–‡æ¡£æ‘˜è¦æè¿°ï¼Œ100-200å­—ï¼Œæ¦‚æ‹¬æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹å’Œä»·å€¼"
    )
    category: str = Field(
        description="æ–‡æ¡£åˆ†ç±»ï¼Œå¦‚ï¼šå¸‚åœºåˆ†æã€ä¿¡ç”¨è¯„çº§ã€è¡Œä¸šç ”ç©¶ã€æ”¿ç­–è§£è¯»ã€å®è§‚ç»æµã€æŠ•èµ„åˆ†æç­‰"
    )
    tags: List[str] = Field(
        description="æ–‡æ¡£æ ‡ç­¾ï¼Œ3-8ä¸ªå…³é”®è¯ï¼Œç”¨äºæœç´¢å’Œåˆ†ç±»",
        min_items=3,
        max_items=8
    )
    key_insights: List[str] = Field(
        description="æ ¸å¿ƒæ´å¯Ÿï¼Œ3-5ä¸ªè¦ç‚¹ï¼Œæ–‡æ¡£çš„å…³é”®ç»“è®ºæˆ–å‘ç°",
        min_items=1,
        max_items=5
    )
    target_audience: Optional[str] = Field(
        default=None,
        description="ç›®æ ‡è¯»è€…ç¾¤ä½“"
    )


def count_document_words(document: str) -> int:
    """
    ç»Ÿè®¡æ–‡æ¡£å­—æ•°ï¼ˆæ’é™¤å›¾è¡¨ã€å¼•ç”¨ã€ä»£ç å—ç­‰ï¼‰

    ç»Ÿè®¡è§„åˆ™ï¼š
    - ä¸­æ–‡ï¼šæŒ‰å­—ç¬¦æ•°ç»Ÿè®¡
    - è‹±æ–‡ï¼šæŒ‰å•è¯æ•°ç»Ÿè®¡
    - æ’é™¤ï¼šMarkdown å›¾ç‰‡ã€è¡¨æ ¼ã€ä»£ç å—ã€å¼•ç”¨å—ã€é“¾æ¥ç­‰

    Args:
        document: æ–‡æ¡£å†…å®¹

    Returns:
        æœ‰æ•ˆå­—æ•°
    """
    if not document:
        return 0

    text = document

    # ç§»é™¤ä»£ç å— ```...```
    text = re.sub(r'```[\s\S]*?```', '', text)

    # ç§»é™¤è¡Œå†…ä»£ç  `...`
    text = re.sub(r'`[^`]+`', '', text)

    # ç§»é™¤å›¾ç‰‡ ![alt](url) æˆ– ![alt][ref]
    text = re.sub(r'!\[[^\]]*\]\([^)]*\)', '', text)
    text = re.sub(r'!\[[^\]]*\]\[[^\]]*\]', '', text)

    # ç§»é™¤é“¾æ¥ä½†ä¿ç•™æ–‡æœ¬ [text](url) -> text
    text = re.sub(r'\[([^\]]*)\]\([^)]*\)', r'\1', text)

    # ç§»é™¤è¡¨æ ¼ï¼ˆç®€å•å¤„ç†ï¼šç§»é™¤åŒ…å« | çš„è¡Œï¼‰
    lines = text.split('\n')
    lines = [line for line in lines if not re.match(r'^\s*\|.*\|\s*$', line)]
    lines = [line for line in lines if not re.match(r'^\s*[\-\|:]+\s*$', line)]
    text = '\n'.join(lines)

    # ç§»é™¤å¼•ç”¨å— > ...
    text = re.sub(r'^>\s*.*$', '', text, flags=re.MULTILINE)

    # ç§»é™¤å‚è€ƒæ–‡çŒ®/å¼•ç”¨æ ‡è®° [1], [^1], [[1]] ç­‰
    text = re.sub(r'\[\^?\d+\]', '', text)
    text = re.sub(r'\[\[\d+\]\]', '', text)

    # ç§»é™¤ Markdown æ ‡é¢˜æ ‡è®° #
    text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)

    # ç§»é™¤ç²—ä½“/æ–œä½“æ ‡è®°
    text = re.sub(r'\*+([^*]+)\*+', r'\1', text)
    text = re.sub(r'_+([^_]+)_+', r'\1', text)

    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()

    # ç»Ÿè®¡å­—æ•°
    # ä¸­æ–‡å­—ç¬¦
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    chinese_count = len(chinese_chars)

    # è‹±æ–‡å•è¯ï¼ˆè¿ç»­çš„å­—æ¯æ•°å­—åºåˆ—ï¼‰
    # å…ˆç§»é™¤ä¸­æ–‡å­—ç¬¦å†ç»Ÿè®¡è‹±æ–‡
    text_without_chinese = re.sub(r'[\u4e00-\u9fff]', ' ', text)
    english_words = re.findall(r'[a-zA-Z0-9]+', text_without_chinese)
    english_count = len(english_words)

    total_count = chinese_count + english_count

    logger.debug(f"å­—æ•°ç»Ÿè®¡: ä¸­æ–‡ {chinese_count} + è‹±æ–‡ {english_count} = {total_count}")

    return total_count


def calculate_reading_time(word_count: int, wpm: int = 300) -> int:
    """
    è®¡ç®—é¢„ä¼°é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰

    Args:
        word_count: å­—æ•°
        wpm: æ¯åˆ†é’Ÿé˜…è¯»å­—æ•°ï¼Œé»˜è®¤300ï¼ˆä¸­æ–‡å¹³å‡é˜…è¯»é€Ÿåº¦ï¼‰

    Returns:
        é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ï¼Œè‡³å°‘1åˆ†é’Ÿ
    """
    minutes = max(1, round(word_count / wpm))
    return minutes


async def document_finalizer(state: DocumentState) -> Dict[str, Any]:
    """
    æ–‡æ¡£æœ€ç»ˆåŒ–èŠ‚ç‚¹ - æå–å…ƒæ•°æ®å¹¶å®Œæˆç»Ÿè®¡

    Args:
        state: DocumentStateï¼Œéœ€è¦åŒ…å« document å­—æ®µ

    Returns:
        {
            "document_metadata": {
                "title": str,
                "description": str,
                "category": str,
                "tags": str,  # é€—å·åˆ†éš”
                "word_count": int,
                "estimated_reading_time": int,
                "key_insights": list,
                "target_audience": str,
                "status": "completed"
            }
        }
    """
    logger.info("\nğŸ“‹ [Document Finalizer] æå–æ–‡æ¡£å…ƒæ•°æ®...")

    document = state.get("document", "")
    outline = state.get("document_outline")

    if not document:
        logger.warning("  âš ï¸ æ–‡æ¡£å†…å®¹ä¸ºç©º")
        return {
            "document_metadata": {
                "status": "failed",
                "error": "Document content is empty"
            }
        }

    # === 1. ç»Ÿè®¡å­—æ•° ===
    logger.info("  â†³ ç»Ÿè®¡æ–‡æ¡£å­—æ•°...")
    word_count = count_document_words(document)
    reading_time = calculate_reading_time(word_count)
    logger.info(f"    - æœ‰æ•ˆå­—æ•°: {word_count:,}")
    logger.info(f"    - é¢„ä¼°é˜…è¯»: {reading_time} åˆ†é’Ÿ")

    # === 2. ä½¿ç”¨ LLM æå–ç»“æ„åŒ–å…ƒæ•°æ® ===
    logger.info("  â†³ è°ƒç”¨ LLM æå–å…ƒæ•°æ®...")

    try:
        llm = init_chat_model(
            "deepseek:deepseek-chat",
            temperature=0.3,
        )

        # ä½¿ç”¨ with_structured_output
        structured_llm = llm.with_structured_output(DocumentMetadata)

        # æ„å»ºæç¤ºè¯
        # å–æ–‡æ¡£å‰3000å­—ä½œä¸ºåˆ†ææ ·æœ¬ï¼ˆé¿å… token è¿‡é•¿ï¼‰
        document_sample = document[:3000] if len(document) > 3000 else document

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæå–å…³é”®å…ƒæ•°æ®ä¿¡æ¯ã€‚
        
        åˆ†æè¦æ±‚ï¼š
        1. description: æ’°å†™100-200å­—çš„æ‘˜è¦ï¼Œæ¦‚æ‹¬æ ¸å¿ƒå†…å®¹å’Œä»·å€¼
        2. category: é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»ï¼ˆå¸‚åœºåˆ†æ/ä¿¡ç”¨è¯„çº§/è¡Œä¸šç ”ç©¶/æ”¿ç­–è§£è¯»/å®è§‚ç»æµ/æŠ•èµ„åˆ†æ/ä¼ä¸šè°ƒç ”/è´¢åŠ¡åˆ†æ/å…¶ä»–ï¼‰
        3. tags: æå–3-8ä¸ªå…³é”®æ ‡ç­¾ï¼Œä¾¿äºæœç´¢å’Œåˆ†ç±»
        4. key_insights: æç‚¼3-5ä¸ªæ ¸å¿ƒæ´å¯Ÿæˆ–ç»“è®º
        5. target_audience: è¯†åˆ«ç›®æ ‡è¯»è€…ç¾¤ä½“"""

        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å¹¶æå–å…ƒæ•°æ®ï¼š
        
        æ–‡æ¡£æ ‡é¢˜ï¼š{outline.title if outline else "æœªçŸ¥"}
        
        æ–‡æ¡£å†…å®¹ï¼ˆèŠ‚é€‰ï¼‰ï¼š
        {document_sample}
        
        è¯·æ ¹æ®ä¸Šè¿°å†…å®¹ç”Ÿæˆç»“æ„åŒ–çš„å…ƒæ•°æ®ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        metadata_result: DocumentMetadata = await structured_llm.ainvoke(messages)

        # æ„å»ºæœ€ç»ˆå…ƒæ•°æ®
        document_metadata = {
            "title": metadata_result.title,
            "description": metadata_result.description,
            "category": metadata_result.category,
            "tags": ",".join(metadata_result.tags),  # è½¬ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²
            "word_count": word_count,
            "estimated_reading_time": reading_time,
            "key_insights": metadata_result.key_insights,
            "target_audience": metadata_result.target_audience,
            "status": "completed"
        }

        logger.success("  âœ“ å…ƒæ•°æ®æå–å®Œæˆ")
        logger.info(f"    - æ ‡é¢˜: {metadata_result.title}")
        logger.info(f"    - åˆ†ç±»: {metadata_result.category}")
        logger.info(f"    - æ ‡ç­¾: {', '.join(metadata_result.tags)}")
        logger.info(f"    - å­—æ•°: {word_count:,}")
        logger.info(f"    - é˜…è¯»æ—¶é—´: {reading_time} åˆ†é’Ÿ\n")

        return {"document_metadata": document_metadata}

    except Exception as e:
        logger.error(f"  âŒ å…ƒæ•°æ®æå–å¤±è´¥: {e}")

        # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸºç¡€ä¿¡æ¯
        fallback_metadata = {
            "title": outline.title if outline else "æœªå‘½åæ–‡æ¡£",
            "description": f"åŸºäº {outline.title if outline else 'ä¸»é¢˜'} çš„åˆ†ææŠ¥å‘Š" if outline else "",
            "category": "å…¶ä»–",
            "tags": "",
            "word_count": word_count,
            "estimated_reading_time": reading_time,
            "key_insights": [],
            "target_audience": None,
            "status": "partial",
            "error": str(e)
        }

        return {"document_metadata": fallback_metadata}
